{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c4cb150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install requests tqdm sentence-transformers scikit-learn pandas\n",
    "# %pip install sentence-transformers\n",
    "# pip install huggingface_hub[hf_xet]\n",
    "# %pip install tf-keras\n",
    "# Opcional si activas clasificador cero-shot:\n",
    "# %pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5857560",
   "metadata": {},
   "source": [
    "Evaluación de Patentabilidad: Novedad, Nivel Inventivo y Aplicación Industrial\n",
    "\n",
    "Resumen:\n",
    "- Busca arte previo en:\n",
    "  - OpenAlex (literatura científica)\n",
    "  - PatentsView (patentes USA)\n",
    "- Calcula similitud semántica con embeddings multilingües\n",
    "- Emite reporte con:\n",
    "  1) Novedad (score y referencias más similares)\n",
    "  2) Nivel inventivo (índice de obviedad y análisis de clustering)\n",
    "  3) Aplicación industrial (CPC sugerido + clasificador por etiquetas opcional)\n",
    "\n",
    "Avisos:\n",
    "- Términos de Uso de las APIs.\n",
    "- Ajuste umbrales.\n",
    "- Valida resultados con expertos en PI.\n",
    "\n",
    "Parámetros clave:\n",
    "--max_results_openalex, --max_results_patents, --top_k, --threshold_novelty, --threshold_obvious_mean, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30f5dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import datetime as dt\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59a3fedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saraujo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\saraujo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except ImportError as e:\n",
    "    raise SystemExit(\"Falta sentence-transformers. Instala con: pip install sentence-transformers\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9635a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gets_api import normalize_text, search_openalex, search_patentsview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b243fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23a715dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Reference:\n",
    "    source: str                       # \"openalex\" | \"patentsview\"\n",
    "    id: str\n",
    "    title: str\n",
    "    abstract: str\n",
    "    url: Optional[str]\n",
    "    date: Optional[str]\n",
    "    authors_or_assignees: Optional[str] = None\n",
    "    cpc_sections: List[str] = field(default_factory=list)\n",
    "    cpc_groups: List[str] = field(default_factory=list)\n",
    "    score: Optional[float] = None     # Similaridad final con el proyecto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf51490d",
   "metadata": {},
   "source": [
    "# Embeddings y similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "799d47ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedder(model_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\") -> SentenceTransformer:\n",
    "    \"\"\"\n",
    "    Carga un modelo de embedding de oraciones.\n",
    "    Modelo multilingüe recomendado para español/inglés.\n",
    "    Alternativas:\n",
    "      - sentence-transformers/distiluse-base-multilingual-cased-v2\n",
    "      - sentence-transformers/all-MiniLM-L6-v2 (monolingüe inglés)\n",
    "    \"\"\"\n",
    "    return SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0dbfe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts(embedder: SentenceTransformer, texts: List[str], batch_size: int = 32, show_progress_bar: bool = True) -> np.ndarray:\n",
    "    \"\"\" \n",
    "    Embedding de una lista de textos. \n",
    "    Normaliza los embeddings para facilitar el cálculo de similitud coseno.\n",
    "    \"\"\"\n",
    "    return np.asarray(embedder.encode(texts, batch_size=batch_size, convert_to_numpy=True, show_progress_bar=show_progress_bar, normalize_embeddings=True))\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Similitud coseno entre dos conjuntos de embeddings normalizados. \"\"\"\n",
    "    return cosine_similarity(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0cc4fb",
   "metadata": {},
   "source": [
    "# Heurísticas de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "335abd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_novelty(max_sim: float, threshold_novelty: float = 0.80, borderline_band: Tuple[float, float] = (0.65, 0.80)) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluación de novedad basada en la similitud máxima con el corpus previo.\n",
    "    Si la similitud es mas baja que el umbral -> Novedad probable.\n",
    "\n",
    "    Novedad alta ~ menor similitud con arte previo. Aquí invertimos la lógica:\n",
    "    - Si max_sim >= threshold_novelty -> Probable NO novedad (muy parecido).\n",
    "    - Si entre borderline -> Revisar (posible colisión parcial).\n",
    "    - Si por debajo del borde -> Novedad probable.\n",
    "    \"\"\"\n",
    "    lower, upper = borderline_band\n",
    "    if max_sim >= threshold_novelty:\n",
    "        status = \"No novedoso (colisión probable)\"\n",
    "        risk = \"ALTO\"\n",
    "    elif max_sim >= lower:\n",
    "        status = \"Revisar (zonas grises, semejanza moderada)\"\n",
    "        risk = \"MEDIO\"\n",
    "    else:\n",
    "        status = \"Novedad probable\"\n",
    "        risk = \"BAJO\"\n",
    "    novelty_score = 1.0 - max_sim  # más alto = más novedoso\n",
    "    return {\n",
    "        \"status\": status,\n",
    "        \"risk\": risk,\n",
    "        \"max_similarity\": round(max_sim, 4),\n",
    "        \"novelty_score\": round(novelty_score, 4),\n",
    "        \"thresholds\": {\n",
    "            \"no_novedad_si_sim_ge\": threshold_novelty,\n",
    "            \"zona_gris\": borderline_band\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "772ad602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_inventive_step(\n",
    "    top_k_sims: List[float],\n",
    "    cpc_groups_of_top: List[List[str]],\n",
    "    obvious_mean_threshold: float = 0.60,\n",
    "    multi_ref_obvious_threshold: float = 0.55,\n",
    "    min_refs_for_obviousness: int = 3\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluación de nivel inventivo (no-obviedad).\n",
    "    Si la similitud es alta con varios documentos o si hay múltiples referencias fuertes de diferentes grupos CPC,\n",
    "    puede indicar obviedad.\n",
    "    \n",
    "\n",
    "    Heurística de nivel inventivo (no-obviedad). Ideas:\n",
    "    - Si el promedio de top_k es alto, sugiere obviedad (mucha proximidad a varios documentos).\n",
    "    - Si existen >= min_refs referencias con sim >= multi_ref_obvious_threshold provenientes de grupos CPC diferentes,\n",
    "      puede indicar combinación obvia (riesgo para nivel inventivo).\n",
    "    - Resultado: Alto/Medio/Bajo riesgo de falta de nivel inventivo.\n",
    "    \"\"\"\n",
    "    if not top_k_sims:\n",
    "        return {\n",
    "            \"status\": \"Insuficiente evidencia (pocas coincidencias)\",\n",
    "            \"risk\": \"DESCONOCIDO\",\n",
    "            \"mean_topk_similarity\": None,\n",
    "            \"explanation\": \"No se hallaron suficientes vecinos para evaluar.\"\n",
    "        }\n",
    "\n",
    "    mean_sim = float(np.mean(top_k_sims))\n",
    "    high_cluster = mean_sim >= obvious_mean_threshold # si el promedio de la similitud de los top-k mayor al umbral\n",
    "\n",
    "    # Diversidad CPC entre los top-k\n",
    "    flat_groups = [g for groups in cpc_groups_of_top for g in (groups or [])]\n",
    "    unique_groups = set(flat_groups)\n",
    "    strong_refs = sum(1 for s in top_k_sims if s >= multi_ref_obvious_threshold) # suma de la cantidad de referencias fuertes (similitud mas alta que el umbral)\n",
    "    diverse = len(unique_groups) >= min(3, len(top_k_sims))  # diversidad mínima, si hay al menos 3 grupos CPC diferentes entre los top-k\n",
    "\n",
    "    if high_cluster or (strong_refs >= min_refs_for_obviousness and diverse):\n",
    "        status = \"Riesgo: nivel inventivo bajo (obviedad probable)\"\n",
    "        risk = \"ALTO\"\n",
    "    elif mean_sim >= (obvious_mean_threshold - 0.1) or strong_refs >= (min_refs_for_obviousness - 1):\n",
    "        status = \"Riesgo intermedio (posible obviedad)\"\n",
    "        risk = \"MEDIO\"\n",
    "    else:\n",
    "        status = \"Nivel inventivo probable (no-obvio)\"\n",
    "        risk = \"BAJO\"\n",
    "\n",
    "    return {\n",
    "        \"status\": status,\n",
    "        \"risk\": risk,\n",
    "        \"mean_topk_similarity\": round(mean_sim, 4),\n",
    "        \"strong_ref_count\": strong_refs,\n",
    "        \"unique_cpc_groups\": list(sorted(unique_groups)),\n",
    "        \"explanation\": \"Promedio de similitudes y diversidad CPC considerados.\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "387f4763",
   "metadata": {},
   "outputs": [],
   "source": [
    "CPC_SECTION_MAP = {\n",
    "    \"A\": \"Necesidades humanas\",\n",
    "    \"B\": \"Técnicas industriales; transporte\",\n",
    "    \"C\": \"Química; metalurgia\",\n",
    "    \"D\": \"Textiles; papel\",\n",
    "    \"E\": \"Construcciones fijas\",\n",
    "    \"F\": \"Ingeniería mecánica; iluminación; calefacción; armas; voladura\",\n",
    "    \"G\": \"Física (incluye computación)\",\n",
    "    \"H\": \"Electricidad\",\n",
    "    \"Y\": \"Tecnologías emergentes o transversales\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b08081e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDUSTRIAL_KEYWORDS = [\n",
    "    # Originales (Manufactura/Proceso)\n",
    "    \"proceso\", \"procedimiento\", \"método\", \"fabricación\", \"producción\",\n",
    "    \"dispositivo\", \"sistema\", \"aparato\", \"planta\", \"línea de\", \"automatización\",\n",
    "    \"ensamblaje\", \"tratamiento\", \"reactor\", \"módulo\", \"unidad\", \"industrial\", \"piloto\",\n",
    "    \n",
    "    # Nuevas (Software/Digital)\n",
    "    \"software\", \"plataforma\", \"algoritmo\", \"hardware\", \"circuito\", \"red\",\n",
    "    \"interfaz\", \"base de datos\",\n",
    "    \n",
    "    # Nuevas (Química/Bio)\n",
    "    \"composición\", \"formulación\", \"compuesto\", \"cultivo\", \"secuencia\",\n",
    "    \n",
    "    # Nuevas (General/Comercial)\n",
    "    \"manufactura\", \"escalado\", \"comercial\", \"uso de\", \"aplicación de\", \"maquinaria\"\n",
    "]\n",
    "\n",
    "DEFAULT_SEMANTIC_LABELS = [\n",
    "    # Originales\n",
    "    \"biotecnología\", \"química\", \"farmacéutica\", \"software\", \"IA/ML\", \"robótica\",\n",
    "    \"energía\", \"mecánica\", \"eléctrica\", \"electrónica\", \"telecomunicaciones\",\n",
    "    \"agricultura\", \"materiales\", \"médico\", \"construcción\", \"transporte\",\n",
    "    \"minería\", \"textil\", \"alimentos\",\n",
    "    \n",
    "    # Nuevas (Digital/IA)\n",
    "    \"inteligencia artificial\", \"aprendizaje automático\", \"procesamiento de lenguaje natural (NLP)\",\n",
    "    \"visión por computadora\", \"ciberseguridad\", \"blockchain\", \"fintech\",\n",
    "    \"internet de las cosas (IoT)\", \"realidad aumentada (AR/VR)\", \"gemelos digitales\",\n",
    "    \n",
    "    # Nuevas (Bio/Materiales)\n",
    "    \"bioinformática\", \"terapia génica\", \"edición genética (CRISPR)\", \"nanotecnología\",\n",
    "    \"materiales compuestos\", \"biomateriales\",\n",
    "    \n",
    "    # Nuevas (Industria/Energía)\n",
    "    \"energías renovables\", \"solar\", \"eólica\", \"hidrógeno verde\",\n",
    "    \"almacenamiento de energía\", \"baterías\", \"vehículos autónomos\",\n",
    "    \"vehículos eléctricos\", \"impresión 3D\", \"agrotecnología (agritech)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1c08e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_industrial_app(\n",
    "    project_text: str,\n",
    "    proj_vec: np.ndarray,                     # Vector del proyecto\n",
    "    nearest_patents: List[Reference],\n",
    "    semantic_labels_list: List[str],          # Lista de etiquetas (strings)\n",
    "    semantic_labels_vecs: np.ndarray,         # Vectores de esas etiquetas\n",
    "    min_keyword_hits: int = 1\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Clasificador de aplicabilidad industrial:\n",
    "    - Evidencia textual: presencia de términos industriales en el proyecto.\n",
    "    - Sugerencia CPC: mayorías por sección/grupo en patentes más cercanas.\n",
    "    - Etiquetas semánticas: sugerencias simples por similitud de palabras clave (opcional).\n",
    "\n",
    "    nearest_patents: patentes con similitud más alta\n",
    "    semantic_labels: etiquetas semánticas para sugerencias simples\n",
    "\n",
    "    Nota: Puede ampliarse con cero-shot NLI (xnli) si se requiere.\n",
    "    CPC_SECTION_MAP = traduce la letra de sección a una descripción humana\n",
    "    \"\"\"\n",
    "    text = project_text.lower()\n",
    "    # --- 1. EVIDENCIA TEXTUAL (LÓGICA MEJORADA) ---\n",
    "    # Búsqueda con límites de palabra (\\b) para evitar falsos positivos\n",
    "    keyword_hits = []\n",
    "    for kw in INDUSTRIAL_KEYWORDS:\n",
    "        # Busca la palabra exacta (ej. \"red\" y no \"alrededor\")\n",
    "        if re.search(r'\\b' + re.escape(kw) + r'\\b', text):\n",
    "            keyword_hits.append(kw)\n",
    "            \n",
    "    evidence = len(keyword_hits) >= min_keyword_hits\n",
    "\n",
    "    # --- 2. ANÁLISIS CPC ---\n",
    "    # Mayoría CPC por secciones y grupos\n",
    "    section_counts: Dict[str, int] = {}\n",
    "    group_counts: Dict[str, int] = {}\n",
    "    for ref in nearest_patents:\n",
    "        for s in (ref.cpc_sections or []):\n",
    "            section_counts[s] = section_counts.get(s, 0) + 1\n",
    "        for g in (ref.cpc_groups or []):\n",
    "            group_counts[g] = group_counts.get(g, 0) + 1\n",
    "\n",
    "    print(\"section_counts\", section_counts)\n",
    "    print(\"group_counts\", group_counts)\n",
    "    top_sections = sorted(section_counts.items(), key=lambda x: x[1], reverse=True)[:5] # Top 5 secciones CPC\n",
    "    top_groups = sorted(group_counts.items(), key=lambda x: x[1], reverse=True)[:10] # Top 10 grupos CPC\n",
    "\n",
    "\n",
    "    # --- 3. ETIQUETAS SEMÁNTICAS (LÓGICA MEJORADA) ---\n",
    "    \"\"\"\n",
    "    Etiquetas sugeridas de semántica simples: conteo de palabras\n",
    "    Reemplazamos el conteo de palabras por similitud semántica\n",
    "    verifica similaridad entre el proyecto y las etiquetas semánticas\n",
    "    \"\"\"\n",
    "    sem_suggestions: List[Tuple[str, float]] = []\n",
    "    if semantic_labels_list and semantic_labels_vecs.any():\n",
    "        # Calcular similitud coseno entre el proyecto (1,D) y las etiquetas (N,D)\n",
    "        sims_labels = cosine_sim(proj_vec, semantic_labels_vecs).flatten()\n",
    "        \n",
    "        # Obtener los 5 mejores índices\n",
    "        top_indices = sims_labels.argsort()[-5:][::-1]\n",
    "        \n",
    "        for i in top_indices:\n",
    "            score = round(float(sims_labels[i]), 4)\n",
    "            # Opcional: filtrar similitudes muy bajas si se desea\n",
    "            # if score > 0.1: \n",
    "            sem_suggestions.append((semantic_labels_list[i], score))\n",
    "\n",
    "    # --- 4. REPORTE (SIN CAMBIOS) ---\n",
    "    print(\"Evidence:\", evidence)\n",
    "    print(\"Top CPC Sections:\", top_sections)\n",
    "    status = \"Aplicación industrial probable\" if evidence or top_sections else \"Revisar (evidencia limitada)\"\n",
    "    return {\n",
    "        \"status\": status,\n",
    "        \"evidence_keywords\": keyword_hits,\n",
    "        \"top_cpc_sections\": [(s, c, CPC_SECTION_MAP.get(s, \"\")) for s, c in top_sections], # EJ: (\"G\", 5, \"Física (incluye computación)\")\n",
    "        \"top_cpc_groups\": top_groups,\n",
    "        \"semantic_labels_suggested\": sem_suggestions # EJ: [(\"IA/ML\", 0.75), ...]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7c0ebb",
   "metadata": {},
   "source": [
    "# Pipeline principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cd8ca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus_strings(refs: List[Reference]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Para embeddings: concatenar título + abstract de cada referencia.\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    for r in refs:\n",
    "        t = normalize_text(r.title)\n",
    "        a = normalize_text(r.abstract)\n",
    "        if t and a:\n",
    "            corpus.append(f\"{t}. {a}\")\n",
    "        else:\n",
    "            corpus.append(t or a)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5846418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_project(\n",
    "    project_text: tuple[str, str],\n",
    "    max_results_openalex: int = 50,\n",
    "    max_results_patents: int = 100,\n",
    "    embedder_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    top_k: int = 10,\n",
    "    novelty_threshold: float = 0.80,\n",
    "    borderline_band: Tuple[float, float] = (0.65, 0.80),\n",
    "    obvious_mean_threshold: float = 0.60,\n",
    "    multi_ref_obvious_threshold: float = 0.55,\n",
    "    min_refs_for_obviousness: int = 3,\n",
    "    semantic_labels: Optional[List[str]] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Orquestación completa: búsqueda, embeddings, similitudes y reporte.\n",
    "    \"\"\"\n",
    "    project_title_text = normalize_text(project_text[0])\n",
    "    project_abstract_text = normalize_text(project_text[1])\n",
    "    project_completo_text = f\"{project_title_text}{'' if project_title_text[-1] == '.' else '.'} {project_abstract_text}\"\n",
    "\n",
    "    if not project_text:\n",
    "        raise ValueError(\"El texto del proyecto está vacío.\")\n",
    "    \n",
    "    print(\"project_title_text\", project_title_text)\n",
    "    print(\"project_abstract_text\", project_abstract_text)\n",
    "    print(\"project_completo_text\", project_completo_text)\n",
    "\n",
    "    print(\"1) Buscando en OpenAlex...\")\n",
    "    oa_refs = search_openalex(\n",
    "        query=project_title_text,\n",
    "        max_results=max_results_openalex,\n",
    "        per_page=50,\n",
    "        from_year=None,\n",
    "        to_year=None,\n",
    "        lang_filter=None\n",
    "    )\n",
    "\n",
    "    print(f\"   -> {len(oa_refs)} resultados de OpenAlex\")\n",
    "\n",
    "\n",
    "    print(\"2) Buscando en PatentsView...\")\n",
    "    pv_refs = search_patentsview(\n",
    "        query_text=(project_title_text, project_abstract_text),\n",
    "        max_results=max_results_patents,\n",
    "        per_page=100\n",
    "    )\n",
    "    print(f\"   -> {len(pv_refs)} resultados de PatentsView\")\n",
    "\n",
    "    \n",
    "    # Construir corpus\n",
    "    all_refs = oa_refs + pv_refs\n",
    "    corpus = build_corpus_strings(all_refs)\n",
    "    # ← VALIDACIÓN PARA CORPUS VACÍO\n",
    "    if not corpus or len(corpus) == 0:\n",
    "        print(\"⚠️  Advertencia: No se encontraron referencias en las APIs\")\n",
    "        return {\n",
    "            \"timestamp\": dt.datetime.utcnow().isoformat() + \"Z\",\n",
    "            \"input_project_excerpt\": project_completo_text[:600] + (\"...\" if len(project_completo_text) > 600 else \"\"),\n",
    "            \"error\": \"No se encontraron referencias en OpenAlex ni PatentsView\",\n",
    "            \"parameters\": {\n",
    "                \"max_results_openalex\": max_results_openalex,\n",
    "                \"max_results_patents\": max_results_patents,\n",
    "                \"embedder_name\": embedder_name,\n",
    "                \"top_k\": top_k,\n",
    "                \"novelty_threshold\": novelty_threshold,\n",
    "                \"borderline_band\": borderline_band,\n",
    "                \"obvious_mean_threshold\": obvious_mean_threshold,\n",
    "                \"multi_ref_obvious_threshold\": multi_ref_obvious_threshold,\n",
    "                \"min_refs_for_obviousness\": min_refs_for_obviousness\n",
    "            },\n",
    "            \"modules\": {\n",
    "                \"1_novedad\": {\n",
    "                    \"status\": \"Sin datos para evaluar\",\n",
    "                    \"risk\": \"DESCONOCIDO\",\n",
    "                    \"max_similarity\": 0.0,\n",
    "                    \"novelty_score\": 1.0,\n",
    "                    \"note\": \"No se encontraron referencias para comparar\"\n",
    "                },\n",
    "                \"2_nivel_inventivo\": {\n",
    "                    \"status\": \"Sin datos para evaluar\",\n",
    "                    \"risk\": \"DESCONOCIDO\",\n",
    "                    \"mean_topk_similarity\": None,\n",
    "                    \"explanation\": \"No se encontraron referencias para comparar\"\n",
    "                },\n",
    "                \"3_aplicacion_industrial\": {\n",
    "                    \"status\": \"Evaluación basada solo en palabras clave\",\n",
    "                    \"evidence_keywords\": [kw for kw in INDUSTRIAL_KEYWORDS if kw in project_completo_text.lower()],\n",
    "                    \"top_cpc_sections\": [],\n",
    "                    \"top_cpc_groups\": [],\n",
    "                    \"semantic_labels_suggested\": []\n",
    "                }\n",
    "            },\n",
    "            \"top_references\": []\n",
    "        }\n",
    "\n",
    "    # Embeddings\n",
    "    print(\"3) Generando embeddings...\")\n",
    "    \n",
    "    IS_E5 = \"e5\" in embedder_name.lower()\n",
    "    proj_input = [f\"query: {project_completo_text}\"] if IS_E5 else [project_completo_text]\n",
    "    corpus_inputs = [f\"passage: {t}\" for t in corpus] if IS_E5 else corpus\n",
    "    print(\"Datos listos para embeddings\")\n",
    "\n",
    "\n",
    "    embedder = load_embedder(embedder_name)\n",
    "    print(\"   Modelo de embedding cargado:\", embedder_name)\n",
    "    print(\"Tamaño del proyecto:\", len(project_completo_text))\n",
    "    proj_vec = embed_texts(embedder, proj_input) # Embedding del proyecto\n",
    "    print(\"   Embedding del proyecto generado.\")\n",
    "    print(\"Tamaño del corpus:\", len(corpus_inputs))\n",
    "    corpus_vecs = embed_texts(embedder, corpus_inputs) # Embeddings del corpus\n",
    "    print(\"   Embeddings del corpus generados.\")\n",
    "\n",
    "    # Determina qué etiquetas usar (las del argumento o las por defecto)\n",
    "    labels_to_use = semantic_labels or DEFAULT_SEMANTIC_LABELS\n",
    "    # Pre-calcula los embeddings de las etiquetas una sola vez\n",
    "    print(\"   Generando embeddings de etiquetas semánticas...\")\n",
    "    labels_vecs = embed_texts(embedder, labels_to_use, batch_size=8, show_progress_bar=False)\n",
    "\n",
    "    # Similitudes\n",
    "    sims = cosine_sim(proj_vec, corpus_vecs).flatten() # Similitudes coseno\n",
    "    for ref, s in zip(all_refs, sims):\n",
    "        ref.score = float(s)\n",
    "\n",
    "    # Ordenar por similitud\n",
    "    sorted_refs = sorted(all_refs, key=lambda r: r.score or 0.0, reverse=True) # Más similar primero, orden descendente\n",
    "    max_sim = float(max(sims)) if len(sims) else 0.0 # Similitud máxima\n",
    "\n",
    "    # 1) Novedad\n",
    "    novelty = evaluate_novelty(\n",
    "        max_sim=max_sim,\n",
    "        threshold_novelty=novelty_threshold,\n",
    "        borderline_band=borderline_band\n",
    "    )\n",
    "\n",
    "    # 2) Nivel inventivo\n",
    "    top_k_refs = sorted_refs[:min(top_k, len(sorted_refs))]\n",
    "    top_k_sims = [r.score or 0.0 for r in top_k_refs]\n",
    "    top_k_cpc_groups = [r.cpc_groups for r in top_k_refs]\n",
    "    inventive = evaluate_inventive_step(\n",
    "        top_k_sims=top_k_sims,\n",
    "        cpc_groups_of_top=top_k_cpc_groups,\n",
    "        obvious_mean_threshold=obvious_mean_threshold,\n",
    "        multi_ref_obvious_threshold=multi_ref_obvious_threshold,\n",
    "        min_refs_for_obviousness=min_refs_for_obviousness\n",
    "    )\n",
    "\n",
    "    # 3) Aplicación industrial\n",
    "    nearest_patents = [r for r in top_k_refs if r.source == \"patentsview\"]\n",
    "    industrial = classify_industrial_app(\n",
    "        project_text=project_completo_text,\n",
    "        proj_vec=proj_vec,                      # vector del proyecto\n",
    "        nearest_patents=nearest_patents,\n",
    "        semantic_labels_list=labels_to_use,     # lista de strings\n",
    "        semantic_labels_vecs=labels_vecs        # vectores de etiquetas\n",
    "        # min_keyword_hits se usa por defecto\n",
    "    )\n",
    "\n",
    "    # Resumen de referencias principales (mezcla de artículos/patentes)\n",
    "    def ref_to_dict(r: Reference) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"source\": r.source,\n",
    "            \"id\": r.id,\n",
    "            \"title\": r.title,\n",
    "            \"date\": r.date,\n",
    "            \"url\": r.url,\n",
    "            \"score\": round(r.score or 0.0, 4),\n",
    "            \"cpc_sections\": r.cpc_sections,\n",
    "            \"cpc_groups\": r.cpc_groups,\n",
    "            \"by\": r.authors_or_assignees,\n",
    "            \"abstract\": r.abstract\n",
    "        }\n",
    "\n",
    "    top_refs_out = [ref_to_dict(r) for r in top_k_refs]\n",
    "\n",
    "    # Reporte final\n",
    "    report = {\n",
    "        \"timestamp\": dt.datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"input_project_excerpt\": project_completo_text[:600] + (\"...\" if len(project_completo_text) > 600 else \"\"),\n",
    "        \"parameters\": {\n",
    "            \"max_results_openalex\": max_results_openalex,\n",
    "            \"max_results_patents\": max_results_patents,\n",
    "            \"embedder_name\": embedder_name,\n",
    "            \"top_k\": top_k,\n",
    "            \"novelty_threshold\": novelty_threshold,\n",
    "            \"borderline_band\": borderline_band,\n",
    "            \"obvious_mean_threshold\": obvious_mean_threshold,\n",
    "            \"multi_ref_obvious_threshold\": multi_ref_obvious_threshold,\n",
    "            \"min_refs_for_obviousness\": min_refs_for_obviousness\n",
    "        },\n",
    "        \"modules\": {\n",
    "            \"1_novedad\": novelty,\n",
    "            \"2_nivel_inventivo\": inventive,\n",
    "            \"3_aplicacion_industrial\": industrial\n",
    "        },\n",
    "        \"top_references\": top_refs_out\n",
    "    }\n",
    "\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07e82502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_to_markdown(report: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Convierte el reporte a Markdown legible.\n",
    "    \"\"\"\n",
    "    md = []\n",
    "    md.append(f\"# Reporte de Patentabilidad\")\n",
    "    md.append(f\"- Fecha (UTC): {report['timestamp']}\")\n",
    "    md.append(\"\")\n",
    "    md.append(\"## Resumen del Proyecto\")\n",
    "    md.append(f\"{report['input_project_excerpt']}\")\n",
    "    md.append(\"\")\n",
    "    md.append(\"## Parámetros\")\n",
    "    for k, v in report.get(\"parameters\", {}).items():\n",
    "        md.append(f\"- {k}: {v}\")\n",
    "    md.append(\"\")\n",
    "    md.append(\"## 1) Novedad\")\n",
    "    nov = report[\"modules\"][\"1_novedad\"]\n",
    "    md.append(f\"- Estado: {nov['status']}\")\n",
    "    md.append(f\"- Riesgo: {nov['risk']}\")\n",
    "    md.append(f\"- Máxima similitud: {nov['max_similarity']}\")\n",
    "    md.append(f\"- Puntaje de novedad (1 - max_sim): {nov['novelty_score']}\")\n",
    "    md.append(\"\")\n",
    "    md.append(\"## 2) Nivel Inventivo\")\n",
    "    inv = report[\"modules\"][\"2_nivel_inventivo\"]\n",
    "    md.append(f\"- Estado: {inv['status']}\")\n",
    "    md.append(f\"- Riesgo: {inv['risk']}\")\n",
    "    md.append(f\"- Promedio similitud top-k: {inv.get('mean_topk_similarity')}\")\n",
    "    md.append(f\"- Nº refs fuertes: {inv.get('strong_ref_count')}\")\n",
    "    md.append(f\"- CPC grupos únicos: {', '.join(inv.get('unique_cpc_groups', []))}\")\n",
    "    md.append(f\"- Nota: {inv.get('explanation')}\")\n",
    "    md.append(\"\")\n",
    "    md.append(\"## 3) Aplicación Industrial\")\n",
    "    ind = report[\"modules\"][\"3_aplicacion_industrial\"]\n",
    "    md.append(f\"- Estado: {ind['status']}\")\n",
    "    kws = ind.get(\"evidence_keywords\", [])\n",
    "    md.append(f\"- Palabras clave detectadas: {', '.join(kws) if kws else 'Ninguna'}\")\n",
    "    md.append(\"- CPC secciones sugeridas:\")\n",
    "    for s, c, label in ind.get(\"top_cpc_sections\", []):\n",
    "        md.append(f\"  - {s} ({label}): {c}\")\n",
    "    md.append(\"- CPC grupos sugeridos:\")\n",
    "    for g, c in ind.get(\"top_cpc_groups\", []):\n",
    "        md.append(f\"  - {g}: {c}\")\n",
    "    labs = ind.get(\"semantic_labels_suggested\", [])\n",
    "    if labs:\n",
    "        md.append(\"- Etiquetas semánticas sugeridas:\")\n",
    "        for lab, sc in labs:\n",
    "            md.append(f\"  - {lab} (score:{sc})\")\n",
    "    md.append(\"\")\n",
    "    md.append(\"## Top referencias similares\")\n",
    "    for r in report.get(\"top_references\", []):\n",
    "        title = r.get(\"title\") or \"(sin título)\"\n",
    "        url = r.get(\"url\") or \"\"\n",
    "        source = r.get(\"source\")\n",
    "        score = r.get(\"score\")\n",
    "        date = r.get(\"date\") or \"\"\n",
    "        who = r.get(\"by\") or \"\"\n",
    "        md.append(f\"- [{source}] {title} ({date}) | sim={score} | {who} | {url}\")\n",
    "    md.append(\"\")\n",
    "    return \"\\n\".join(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7abc46d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_report_files(report: Dict[str, Any], out_prefix: str = \"reporte_patentabilidad\") -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Guarda JSON y Markdown. Retorna rutas.\n",
    "    \"\"\"\n",
    "    ts = dt.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    json_path = f\"result/{out_prefix}_{ts}.json\"\n",
    "    md_path = f\"result/{out_prefix}_{ts}.md\"\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "    with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report_to_markdown(report))\n",
    "    return json_path, md_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84919826",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Detecting and Characterizing Group Interactions Using 3D Spatial Data to Enhance Human-Robot Engagement\"\n",
    "abstract = \"As robotic systems become increasingly integrated into human environments, it is critical to develop advanced methods that enable them to interpret and respond to complex social dynamics. This work combines a YOLOv8-based human pose estimation approach with 3D Mean Shift clustering for the detection and analysis of behavioral characteristics in social groups, using 3D point clouds generated by the Intel® RealSense™D435i as a cost-effective alternative to LiDAR systems. Our proposed method achieves 97% accuracy in classifying social group geometric configurations (L, C, and I patterns) and demonstrates the value of depth information by reaching 50% precision in 3D group detection using adaptive clustering, significantly outperforming standard 2D approaches. Validation was conducted with 12 participants across 8 experimental scenarios, demonstrating robust estimation of body orientation (40° error), a key indicator for interaction analysis, while head direction estimation presented greater variability (70° error), both measured relative to the depth plane and compared against OptiTrack ground truth data. The framework processes 120 samples at 2–6m distances, achieving 70% torso orientation accuracy at 5m and identifying triadic L-shaped groups with F1-score=0.91. These results enable autonomous robots to quantify group centroids, analyze interaction patterns, and navigate dynamically using real-time convex hull approximations. The integration of accessible 3D perception with efficient processing could enhance human-robot interactions, demonstrating its feasibility in applications such as social robotics, healthcare, care environments, and service industries, where social adaptability and collaborative decision-making are essential.\"\n",
    "text = (title, abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb853e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Nuestro proyecto propone un método de fabricación de...\"\n",
    "input_file = \"./data/....\"\n",
    "\n",
    "labels_file = \"\"\n",
    "max_results_openalex = 50\n",
    "max_results_patents = 100\n",
    "embedder_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "# embedder_name = \"intfloat/multilingual-e5-large\" # small/base/large\n",
    "# embedder_name = \"intfloat/multilingual-e5-base\" # small/base/large\n",
    "\n",
    "# embedder_name = \"intfloat/AI‑Growth‑Lab/PatentSBERTa\" # Puede no rendir tan bien con literatura científica general como los modelos generalistas.\n",
    "# embedder_name = \"BAAI/bge‑m3\" # Más pesado y lento; requiere más memoria.\n",
    "\n",
    "top_k = 10\n",
    "threshold_novelty = 0.8\n",
    "borderline_low = 0.5\n",
    "borderline_high = 0.8\n",
    "borderline_band = (borderline_low, borderline_high)\n",
    "obvious_mean_threshold = 0.6\n",
    "multi_ref_obvious_threshold = 0.55\n",
    "min_refs_for_obviousness = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e820b346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic labels: None\n"
     ]
    }
   ],
   "source": [
    "if text:\n",
    "    project_text = text\n",
    "elif input_file and os.path.exists(input_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        project_text = f.read()\n",
    "else:\n",
    "    raise SystemExit(\"Proporciona --text o --input_file con el contenido del proyecto.\")\n",
    "\n",
    "# etiquetas semánticas para el analisis de aplicación industrial\n",
    "semantic_labels = None\n",
    "if labels_file and os.path.exists(labels_file):\n",
    "    with open(labels_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        semantic_labels = [line.strip() for line in f if line.strip()]\n",
    "print(\"Semantic labels:\", semantic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04cd5c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_title_text Detecting and Characterizing Group Interactions Using 3D Spatial Data to Enhance Human-Robot Engagement\n",
      "project_abstract_text As robotic systems become increasingly integrated into human environments, it is critical to develop advanced methods that enable them to interpret and respond to complex social dynamics. This work combines a YOLOv8-based human pose estimation approach with 3D Mean Shift clustering for the detection and analysis of behavioral characteristics in social groups, using 3D point clouds generated by the Intel® RealSense™D435i as a cost-effective alternative to LiDAR systems. Our proposed method achieves 97% accuracy in classifying social group geometric configurations (L, C, and I patterns) and demonstrates the value of depth information by reaching 50% precision in 3D group detection using adaptive clustering, significantly outperforming standard 2D approaches. Validation was conducted with 12 participants across 8 experimental scenarios, demonstrating robust estimation of body orientation (40° error), a key indicator for interaction analysis, while head direction estimation presented greater variability (70° error), both measured relative to the depth plane and compared against OptiTrack ground truth data. The framework processes 120 samples at 2–6m distances, achieving 70% torso orientation accuracy at 5m and identifying triadic L-shaped groups with F1-score=0.91. These results enable autonomous robots to quantify group centroids, analyze interaction patterns, and navigate dynamically using real-time convex hull approximations. The integration of accessible 3D perception with efficient processing could enhance human-robot interactions, demonstrating its feasibility in applications such as social robotics, healthcare, care environments, and service industries, where social adaptability and collaborative decision-making are essential.\n",
      "project_completo_text Detecting and Characterizing Group Interactions Using 3D Spatial Data to Enhance Human-Robot Engagement. As robotic systems become increasingly integrated into human environments, it is critical to develop advanced methods that enable them to interpret and respond to complex social dynamics. This work combines a YOLOv8-based human pose estimation approach with 3D Mean Shift clustering for the detection and analysis of behavioral characteristics in social groups, using 3D point clouds generated by the Intel® RealSense™D435i as a cost-effective alternative to LiDAR systems. Our proposed method achieves 97% accuracy in classifying social group geometric configurations (L, C, and I patterns) and demonstrates the value of depth information by reaching 50% precision in 3D group detection using adaptive clustering, significantly outperforming standard 2D approaches. Validation was conducted with 12 participants across 8 experimental scenarios, demonstrating robust estimation of body orientation (40° error), a key indicator for interaction analysis, while head direction estimation presented greater variability (70° error), both measured relative to the depth plane and compared against OptiTrack ground truth data. The framework processes 120 samples at 2–6m distances, achieving 70% torso orientation accuracy at 5m and identifying triadic L-shaped groups with F1-score=0.91. These results enable autonomous robots to quantify group centroids, analyze interaction patterns, and navigate dynamically using real-time convex hull approximations. The integration of accessible 3D perception with efficient processing could enhance human-robot interactions, demonstrating its feasibility in applications such as social robotics, healthcare, care environments, and service industries, where social adaptability and collaborative decision-making are essential.\n",
      "1) Buscando en OpenAlex...\n",
      "params {'search': 'Detecting and Characterizing Group Interactions Using 3D Spatial Data to Enhance Human-Robot Engagement', 'per_page': 50, 'page': 1, 'sort': 'relevance_score:desc'}\n",
      "   -> 50 resultados de OpenAlex\n",
      "2) Buscando en PatentsView...\n",
      "URL https://search.patentsview.org/api/v1/patent/\n",
      "params {'q': '{\"_or\": [{\"_text_any\": {\"patent_title\": \"Detecting and Characterizing Group Interactions Using 3D Spatial Data to Enhance Human-Robot Engagement\"}}, {\"_text_any\": {\"patent_abstract\": \"As robotic systems become increasingly integrated into human environments, it is critical to develop advanced methods that enable them to interpret and respond to complex social dynamics. This work combines a YOLOv8-based human pose estimation approach with 3D Mean Shift clustering for the detection and analysis of behavioral characteristics in social groups, using 3D point clouds generated by the Intel\\\\u00ae RealSense\\\\u2122D435i as a cost-effective alternative to LiDAR systems. Our proposed method achieves 97% accuracy in classifying social group geometric configurations (L, C, and I patterns) and demonstrates the value of depth information by reaching 50% precision in 3D group detection using adaptive clustering, significantly outperforming standard 2D approaches. Validation was conducted with 12 participants across 8 experimental scenarios, demonstrating robust estimation of body orientation (40\\\\u00b0 error), a key indicator for interaction analysis, while head direction estimation presented greater variability (70\\\\u00b0 error), both measured relative to the depth plane and compared against OptiTrack ground truth data. The framework processes 120 samples at 2\\\\u20136m distances, achieving 70% torso orientation accuracy at 5m and identifying triadic L-shaped groups with F1-score=0.91. These results enable autonomous robots to quantify group centroids, analyze interaction patterns, and navigate dynamically using real-time convex hull approximations. The integration of accessible 3D perception with efficient processing could enhance human-robot interactions, demonstrating its feasibility in applications such as social robotics, healthcare, care environments, and service industries, where social adaptability and collaborative decision-making are essential.\"}}]}', 'f': '[\"patent_id\", \"patent_title\", \"patent_date\", \"patent_abstract\", \"assignees\"]', 'o': '{\"page\": 1, \"per_page\": 100}'}\n",
      "   -> 0 resultados de PatentsView\n",
      "3) Generando embeddings...\n",
      "Datos listos para embeddings\n",
      "   Modelo de embedding cargado: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Tamaño del proyecto: 1864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Embedding del proyecto generado.\n",
      "Tamaño del corpus: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Embeddings del corpus generados.\n",
      "   Generando embeddings de etiquetas semánticas...\n",
      "section_counts {}\n",
      "group_counts {}\n",
      "Evidence: False\n",
      "Top CPC Sections: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\saraujo\\AppData\\Local\\Temp\\ipykernel_44480\\1322981254.py:178: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": dt.datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    }
   ],
   "source": [
    "report = evaluate_project(\n",
    "    project_text=project_text,\n",
    "    max_results_openalex=max_results_openalex,\n",
    "    max_results_patents=max_results_patents,\n",
    "    embedder_name=embedder_name,\n",
    "    top_k=top_k,\n",
    "    novelty_threshold=threshold_novelty,\n",
    "    borderline_band=(borderline_low, borderline_high),\n",
    "    obvious_mean_threshold=obvious_mean_threshold,\n",
    "    multi_ref_obvious_threshold=multi_ref_obvious_threshold,\n",
    "    min_refs_for_obviousness=min_refs_for_obviousness,\n",
    "    semantic_labels=semantic_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb75e343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['timestamp', 'input_project_excerpt', 'parameters', 'modules', 'top_references'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96e9ba76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1_novedad': {'status': 'No novedoso (colisión probable)',\n",
       "  'risk': 'ALTO',\n",
       "  'max_similarity': 0.9248,\n",
       "  'novelty_score': 0.0752,\n",
       "  'thresholds': {'no_novedad_si_sim_ge': 0.8, 'zona_gris': (0.5, 0.8)}},\n",
       " '2_nivel_inventivo': {'status': 'Riesgo intermedio (posible obviedad)',\n",
       "  'risk': 'MEDIO',\n",
       "  'mean_topk_similarity': 0.5981,\n",
       "  'strong_ref_count': 5,\n",
       "  'unique_cpc_groups': [],\n",
       "  'explanation': 'Promedio de similitudes y diversidad CPC considerados.'},\n",
       " '3_aplicacion_industrial': {'status': 'Revisar (evidencia limitada)',\n",
       "  'evidence_keywords': [],\n",
       "  'top_cpc_sections': [],\n",
       "  'top_cpc_groups': [],\n",
       "  'semantic_labels_suggested': [('robótica', 0.5264),\n",
       "   ('inteligencia artificial', 0.4509),\n",
       "   ('realidad aumentada (AR/VR)', 0.4041),\n",
       "   ('aprendizaje automático', 0.3965),\n",
       "   ('visión por computadora', 0.3367)]}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report[\"modules\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d7bbe8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'openalex',\n",
       "  'id': 'https://openalex.org/W4412822486',\n",
       "  'title': 'Detecting and Characterizing Group Interactions Using 3D Spatial Data to Enhance Human-Robot Engagement',\n",
       "  'date': '2025-06-01',\n",
       "  'url': 'https://doi.org/10.1145/3747393.3747398',\n",
       "  'score': 0.9248,\n",
       "  'cpc_sections': [],\n",
       "  'cpc_groups': [],\n",
       "  'by': 'Steven Araujo Moran (ORCID: https://orcid.org/0009-0005-9635-7307) - ESPOL Polytechnic University, Guayaquil, Ecuador, K. Muñoz - ESPOL Polytechnic University, Guayaquil, Ecuador, L. SALAZAR - ESPOL Polytechnic University, Guayaquil, Ecuador, Boris X. Vintimilla (ORCID: https://orcid.org/0000-0001-8904-0209) - ESPOL Polytechnic University, Guayaquil, Ecuador',\n",
       "  'abstract': ''},\n",
       " {'source': 'openalex',\n",
       "  'id': 'https://openalex.org/W2617211984',\n",
       "  'title': 'Social Eye Gaze in Human-Robot Interaction: A Review',\n",
       "  'date': '2017-03-01',\n",
       "  'url': 'https://doi.org/10.5898/jhri.6.1.admoni',\n",
       "  'score': 0.678,\n",
       "  'cpc_sections': [],\n",
       "  'cpc_groups': [],\n",
       "  'by': 'Henny Admoni (ORCID: https://orcid.org/0000-0003-1796-2196) - Carnegie Mellon University, Brian Scassellati (ORCID: https://orcid.org/0000-0002-7671-7759) - Yale University',\n",
       "  'abstract': \"This article reviews the state of the art in social eye gaze for human-robot interaction (HRI). It establishes three categories of gaze research in HRI, defined by differences in goals and methods: a human-centered approach, which focuses on people's responses to gaze; a design-centered approach, which addresses the features of robot gaze behavior and appearance that improve interaction; and a technology-centered approach, which is concentrated on the computational tools for implementing social eye gaze in robots. This paper begins with background information about gaze research in HRI and ends with a set of open questions.\"},\n",
       " {'source': 'openalex',\n",
       "  'id': 'https://openalex.org/W2099019320',\n",
       "  'title': 'A survey of socially interactive robots',\n",
       "  'date': '2003-02-28',\n",
       "  'url': 'https://doi.org/10.1016/s0921-8890(02)00372-x',\n",
       "  'score': 0.6689,\n",
       "  'cpc_sections': [],\n",
       "  'cpc_groups': [],\n",
       "  'by': 'Terrence Fong - Institut de production et robotique, Ecole Polytechnique Fédérale de Lausanne, CH-1015 Lausanne, Switzerland, Illah Nourbakhsh - The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA, Kerstin Dautenhahn (ORCID: https://orcid.org/0000-0002-9263-3897) - Department of Computer Science, The University of Hertfordshire, College Lane, Hatfield, Hertfordshire AL10 9AB, UK',\n",
       "  'abstract': ''},\n",
       " {'source': 'openalex',\n",
       "  'id': 'https://openalex.org/W2763083925',\n",
       "  'title': 'Robots As Intentional Agents: Using Neuroscientific Methods to Make Robots Appear More Social',\n",
       "  'date': '2017-10-04',\n",
       "  'url': 'https://doi.org/10.3389/fpsyg.2017.01663',\n",
       "  'score': 0.6491,\n",
       "  'cpc_sections': [],\n",
       "  'cpc_groups': [],\n",
       "  'by': 'Eva Wiese (ORCID: https://orcid.org/0000-0002-0322-1250) - Department of Psychology, George Mason University, Fairfax, VA, United States, Giorgio Metta (ORCID: https://orcid.org/0000-0003-0459-4769) - Istituto Italiano di Tecnologia, Genoa, Italy, Agnieszka Wykowska (ORCID: https://orcid.org/0000-0003-3323-7357) - Istituto Italiano di Tecnologia, Genoa, Italy',\n",
       "  'abstract': \"Robots are increasingly envisaged as our future cohabitants. However, while considerable progress has been made in recent years in terms of their technological realization, the ability of robots to interact with humans in an intuitive and social way is still quite limited. An important challenge for social robotics is to determine how to design robots that can perceive the user's needs, feelings, and intentions, and adapt to users over a broad range of cognitive abilities. It is conceivable that if robots were able to adequately demonstrate these skills, humans would eventually accept them as social companions. We argue that the best way to achieve this is using a systematic experimental approach based on behavioral and physiological neuroscience methods such as motion/eye-tracking, electroencephalography, or functional near-infrared spectroscopy embedded in interactive human-robot paradigms. This approach requires understanding how humans interact with each other, how they perform tasks together and how they develop feelings of social connection over time, and using these insights to formulate design principles that make social robots attuned to the workings of the human brain. In this review, we put forward the argument that the likelihood of artificial agents being perceived as social companions can be increased by designing them in a way that they are perceived as intentional agents that activate areas in the human brain involved in social-cognitive processing. We first review literature related to social-cognitive processes and mechanisms involved in human-human interactions, and highlight the importance of perceiving others as intentional agents to activate these social brain areas. We then discuss how attribution of intentionality can positively affect human-robot interaction by (a) fostering feelings of social connection, empathy and prosociality, and by (b) enhancing performance on joint human-robot tasks. Lastly, we describe circumstances under which attribution of intentionality to robot agents might be disadvantageous, and discuss challenges associated with designing social robots that are inspired by neuroscientific principles.\"},\n",
       " {'source': 'openalex',\n",
       "  'id': 'https://openalex.org/W2180635266',\n",
       "  'title': 'A Review of Human Activity Recognition Methods',\n",
       "  'date': '2015-11-16',\n",
       "  'url': 'https://doi.org/10.3389/frobt.2015.00028',\n",
       "  'score': 0.6234,\n",
       "  'cpc_sections': [],\n",
       "  'cpc_groups': [],\n",
       "  'by': 'Michalis Vrigkas (ORCID: https://orcid.org/0000-0001-5888-6949) - Department of Computer Science and Engineering, University of Ioannina, Ioannina, Greece, Christophoros Nikou (ORCID: https://orcid.org/0000-0003-1388-6915) - Department of Computer Science and Engineering, University of Ioannina, Ioannina, Greece, Ioannis A. Kakadiaris (ORCID: https://orcid.org/0000-0002-0591-1079) - Computational Biomedicine Laboratory, Department of Computer Science, University of Houston, Houston, TX, USA',\n",
       "  'abstract': 'Recognizing human activities from video sequences or still images is a challenging task due to problems such as background clutter, partial occlusion, changes in scale, viewpoint, lighting, and appearance. Many applications, including video surveillance systems, human-computer interaction, and robotics for human behavior characterization, require a multiple activity recognition system. In this work, we provide a detailed review of recent and state-of-the-art research advances in the field of human activity classification. We propose a categorization of human activity methodologies and discuss their advantages and limitations. In particular, we divide human activity classification methods into two large categories according to whether they use data from different modalities or not. Then, each of these categories is further analyzed into sub-categories, which reflect how they model human activities and what type of activities they are interested in. Moreover, we provide a comprehensive analysis of the existing, publicly available human activity classification datasets and examine the requirements for an ideal human activity recognition dataset. Finally, we report the characteristics of future research directions and present some open issues on human activity recognition.'},\n",
       " {'source': 'openalex',\n",
       "  'id': 'https://openalex.org/W4388639388',\n",
       "  'title': 'Artificial Intelligence Meets Flexible Sensors: Emerging Smart Flexible Sensing Systems Driven by Machine Learning and Artificial Synapses',\n",
       "  'date': '2023-11-13',\n",
       "  'url': 'https://doi.org/10.1007/s40820-023-01235-x',\n",
       "  'score': 0.5456,\n",
       "  'cpc_sections': [],\n",
       "  'cpc_groups': [],\n",
       "  'by': \"Tianming Sun - College of Materials Science and Engineering, Shanxi Province, Taiyuan University of Technology, Taiyuan, 030024, People's Republic of China., Bin Feng (ORCID: https://orcid.org/0000-0001-5055-9521) - Department of Mechanical Engineering, State Key Laboratory of Tribology in Advanced Equipment, Key Laboratory for Advanced Manufacturing by Materials Processing Technology, Ministry of Education of PR China, Tsinghua University, Beijing, 100084, People's Republic of China., Jinpeng Huo (ORCID: https://orcid.org/0000-0001-7858-0648) - Department of Mechanical Engineering, State Key Laboratory of Tribology in Advanced Equipment, Key Laboratory for Advanced Manufacturing by Materials Processing Technology, Ministry of Education of PR China, Tsinghua University, Beijing, 100084, People's Republic of China., Yu Xiao (ORCID: https://orcid.org/0000-0002-3849-6895) - Department of Mechanical Engineering, State Key Laboratory of Tribology in Advanced Equipment, Key Laboratory for Advanced Manufacturing by Materials Processing Technology, Ministry of Education of PR China, Tsinghua University, Beijing, 100084, People's Republic of China., Wengan Wang (ORCID: https://orcid.org/0000-0002-4235-3686) - Department of Mechanical Engineering, State Key Laboratory of Tribology in Advanced Equipment, Key Laboratory for Advanced Manufacturing by Materials Processing Technology, Ministry of Education of PR China, Tsinghua University, Beijing, 100084, People's Republic of China., Jin Peng (ORCID: https://orcid.org/0000-0003-0611-946X) - Department of Mechanical Engineering, State Key Laboratory of Tribology in Advanced Equipment, Key Laboratory for Advanced Manufacturing by Materials Processing Technology, Ministry of Education of PR China, Tsinghua University, Beijing, 100084, People's Republic of China., Zehua Li (ORCID: https://orcid.org/0009-0000-0120-0256) - Department of Mechanical Engineering, State Key Laboratory of Tribology in Advanced Equipment, Key Laboratory for Advanced Manufacturing by Materials Processing Technology, Ministry of Education of PR China, Tsinghua University, Beijing, 100084, People's Republic of China., Chengjie Du (ORCID: https://orcid.org/0000-0001-7286-9002) - Department of Mechanical Engineering, State Key Laboratory of Tribology in Advanced Equipment, Key Laboratory for Advanced Manufacturing by Materials Processing Technology, Ministry of Education of PR China, Tsinghua University, Beijing, 100084, People's Republic of China., Wenxian Wang - College of Materials Science and Engineering, Shanxi Province, Taiyuan University of Technology, Taiyuan, 030024, People's Republic of China. wangwenxian@tyut.edu.cn., Guisheng Zou - Department of Mechanical Engineering, State Key Laboratory of Tribology in Advanced Equipment, Key Laboratory for Advanced Manufacturing by Materials Processing Technology, Ministry of Education of PR China, Tsinghua University, Beijing, 100084, People's Republic of China. zougsh@tsinghua.edu.cn., Lei Liu (ORCID: https://orcid.org/0000-0002-3368-5136) - Department of Mechanical Engineering, State Key Laboratory of Tribology in Advanced Equipment, Key Laboratory for Advanced Manufacturing by Materials Processing Technology, Ministry of Education of PR China, Tsinghua University, Beijing, 100084, People's Republic of China. liulei@tsinghua.edu.cn.\",\n",
       "  'abstract': 'The recent wave of the artificial intelligence (AI) revolution has aroused unprecedented interest in the intelligentialize of human society. As an essential component that bridges the physical world and digital signals, flexible sensors are evolving from a single sensing element to a smarter system, which is capable of highly efficient acquisition, analysis, and even perception of vast, multifaceted data. While challenging from a manual perspective, the development of intelligent flexible sensing has been remarkably facilitated owing to the rapid advances of brain-inspired AI innovations from both the algorithm (machine learning) and the framework (artificial synapses) level. This review presents the recent progress of the emerging AI-driven, intelligent flexible sensing systems. The basic concept of machine learning and artificial synapses are introduced. The new enabling features induced by the fusion of AI and flexible sensing are comprehensively reviewed, which significantly advances the applications such as flexible sensory systems, soft/humanoid robotics, and human activity monitoring. As two of the most profound innovations in the twenty-first century, the deep incorporation of flexible sensing and AI technology holds tremendous potential for creating a smarter world for human beings.'},\n",
       " {'source': 'openalex',\n",
       "  'id': 'https://openalex.org/W2076748073',\n",
       "  'title': 'A Review of Mobile Robotic Telepresence',\n",
       "  'date': '2013-01-01',\n",
       "  'url': 'https://doi.org/10.1155/2013/902316',\n",
       "  'score': 0.4924,\n",
       "  'cpc_sections': [],\n",
       "  'cpc_groups': [],\n",
       "  'by': 'Annica Kristoffersson (ORCID: https://orcid.org/0000-0002-4368-4751) - Center of Applied Autonomous Sensor Systems, Örebro University, Fakultetsgatan 1, 70182 Örebro, Sweden, Silvia Coradeschi - Center of Applied Autonomous Sensor Systems, Örebro University, Fakultetsgatan 1, 70182 Örebro, Sweden, Amy Loutfi (ORCID: https://orcid.org/0000-0002-3122-693X) - Center of Applied Autonomous Sensor Systems, Örebro University, Fakultetsgatan 1, 70182 Örebro, Sweden',\n",
       "  'abstract': 'Mobile robotic telepresence (MRP) systems incorporate video conferencing equipment onto mobile robot devices which can be steered from remote locations. These systems, which are primarily used in the context of promoting social interaction between people, are becoming increasingly popular within certain application domains such as health care environments, independent living for the elderly, and office environments. In this paper, an overview of the various systems, application areas, and challenges found in the literature concerning mobile robotic telepresence is provided. The survey also proposes a set terminology for the field as there is currently a lack of standard terms for the different concepts related to MRP systems. Further, this paper provides an outlook on the various research directions for developing and enhancing mobile robotic telepresence systems per se, as well as evaluating the interaction in laboratory and field settings. Finally, the survey outlines a number of design implications for the future of mobile robotic telepresence systems for social interaction.'},\n",
       " {'source': 'openalex',\n",
       "  'id': 'https://openalex.org/W2259092398',\n",
       "  'title': 'Toward a Social Psychophysics of Face Communication',\n",
       "  'date': '2017-01-03',\n",
       "  'url': 'https://doi.org/10.1146/annurev-psych-010416-044242',\n",
       "  'score': 0.4757,\n",
       "  'cpc_sections': [],\n",
       "  'cpc_groups': [],\n",
       "  'by': 'Rachael E. Jack (ORCID: https://orcid.org/0000-0003-3687-0799) - Institute of Neuroscience and Psychology, and School of Psychology, University of Glasgow, Glasgow G12 8QB United Kingdom;, Philippe G. Schyns (ORCID: https://orcid.org/0000-0002-8542-7489) - Institute of Neuroscience and Psychology, and School of Psychology, University of Glasgow, Glasgow G12 8QB United Kingdom;',\n",
       "  'abstract': 'As a highly social species, humans are equipped with a powerful tool for social communication—the face. Although seemingly simple, the human face can elicit multiple social perceptions due to the rich variations of its movements, morphology, and complexion. Consequently, identifying precisely what face information elicits different social perceptions is a complex empirical challenge that has largely remained beyond the reach of traditional methods. In the past decade, the emerging field of social psychophysics has developed new methods to address this challenge, with the potential to transfer psychophysical laws of social perception to the digital economy via avatars and social robots. At this exciting juncture, it is timely to review these new methodological developments. In this article, we introduce and review the foundational methodological developments of social psychophysics, present work done in the past decade that has advanced understanding of the face as a tool for social communication, and discuss the major challenges that lie ahead.'},\n",
       " {'source': 'openalex',\n",
       "  'id': 'https://openalex.org/W2787690100',\n",
       "  'title': 'The grand challenges of <i>Science Robotics</i>',\n",
       "  'date': '2018-01-31',\n",
       "  'url': 'https://doi.org/10.1126/scirobotics.aar7650',\n",
       "  'score': 0.4616,\n",
       "  'cpc_sections': [],\n",
       "  'cpc_groups': [],\n",
       "  'by': \"Guang‐Zhong Yang (ORCID: https://orcid.org/0000-0003-4060-4020) - Hamlyn Centre for Robotic Surgery, Imperial College London, London, UK., Jim Bellingham - Center for Marine Robotics, Woods Hole Oceanographic Institution, Woods Hole, MA 02543, USA., Pierre E. Dupont (ORCID: https://orcid.org/0000-0001-7294-640X) - Department of Cardiovascular Surgery, Boston Children's Hospital, Harvard Medical School, Boston, MA 02115, USA., Peer Fischer (ORCID: https://orcid.org/0000-0002-8600-5958) - Micro, Nano, and Molecular Systems Laboratory, Max Planck Institute for Intelligent Systems, Stuttgart, Germany., Luciano Floridi (ORCID: https://orcid.org/0000-0002-5444-2280) - Centre for Practical Ethics, Faculty of Philosophy, University of Oxford, Oxford, UK., Robert J. Full (ORCID: https://orcid.org/0000-0001-8435-5279) - Department of Integrative Biology, University of California, Berkeley, Berkeley, CA 94720, USA., Neil Jacobstein (ORCID: https://orcid.org/0000-0002-3539-4009) - Singularity University, NASA Research Park, Moffett Field, CA 94035, USA., Vijay Kumar - Department of Mechanical Engineering and Applied Mechanics, University of Pennsylvania, Philadelphia, PA 19104, USA., Marcia McNutt (ORCID: https://orcid.org/0000-0003-0117-7716) - National Academy of Sciences, Washington, DC 20418, USA., Robert Merrifield - Hamlyn Centre for Robotic Surgery, Imperial College London, London, UK., Bradley J. Nelson (ORCID: https://orcid.org/0000-0001-9070-6987) - Institute of Robotics and Intelligent Systems, Department of Mechanical and Process Engineering, ETH Zürich, Zurich, Switzerland., Brian Scassellati (ORCID: https://orcid.org/0000-0002-7671-7759) - Department of Computer Science, Yale University, New Haven, CT 06520, USA., Mariarosaria Taddeo (ORCID: https://orcid.org/0000-0002-1181-649X) - Digital Ethics Lab, Oxford Internet Institute, University of Oxford, Oxford, UK., Russell H. Taylor (ORCID: https://orcid.org/0000-0001-6272-1100) - Department of Computer Science, Johns Hopkins University, Baltimore, MD 21218, USA., Manuela Veloso (ORCID: https://orcid.org/0000-0001-6738-238X) - Machine Learning Department, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA., Zhong Lin Wang (ORCID: https://orcid.org/0000-0002-5530-0380) - School of Materials Science and Engineering, Georgia Institute of Technology, Atlanta, GA 30332, USA., Robert J. Wood (ORCID: https://orcid.org/0000-0001-7969-038X) - John A. Paulson School of Engineering and Applied Sciences, Harvard University, Cambridge, MA 02138, USA.\",\n",
       "  'abstract': 'These 10 grand challenges may have major breakthroughs, research, and/or socioeconomic impacts in the next 5 to 10 years.'},\n",
       " {'source': 'openalex',\n",
       "  'id': 'https://openalex.org/W2949263306',\n",
       "  'title': 'Noninvasive neuroimaging enhances continuous neural tracking for robotic device control',\n",
       "  'date': '2019-06-19',\n",
       "  'url': 'https://doi.org/10.1126/scirobotics.aaw6844',\n",
       "  'score': 0.4614,\n",
       "  'cpc_sections': [],\n",
       "  'cpc_groups': [],\n",
       "  'by': 'Bradley J. Edelman (ORCID: https://orcid.org/0000-0002-7502-9620) - Department of Biomedical Engineering, University of Minnesota, Minneapolis, MN 55455, USA., Jianjun Meng (ORCID: https://orcid.org/0000-0003-0813-652X) - Department of Biomedical Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA., Daniel Suma (ORCID: https://orcid.org/0000-0001-9570-311X) - Department of Biomedical Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA., Claire A. Zurn (ORCID: https://orcid.org/0000-0001-8527-1325) - Department of Biomedical Engineering, University of Minnesota, Minneapolis, MN 55455, USA., Eric K. Nagarajan (ORCID: https://orcid.org/0000-0002-4135-9875) - Department of Neuroscience, University of Minnesota, Minneapolis, MN 55455, USA., Bryan Baxter (ORCID: https://orcid.org/0000-0003-3056-0204) - Department of Biomedical Engineering, University of Minnesota, Minneapolis, MN 55455, USA., Christopher C. Cline (ORCID: https://orcid.org/0000-0003-1442-8641) - Department of Biomedical Engineering, University of Minnesota, Minneapolis, MN 55455, USA., Bin He (ORCID: https://orcid.org/0000-0003-2944-8602) - Department of Biomedical Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA.',\n",
       "  'abstract': 'Noninvasive neuroimaging and increased user engagement improve EEG-based neural decoding and facilitate real-time 2D robotic device control.'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report[\"top_references\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f85a1404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte guardado en:\n",
      "- result/reporte_patentabilidad_20251024T185148Z.json\n",
      "- result/reporte_patentabilidad_20251024T185148Z.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saraujo\\AppData\\Local\\Temp\\ipykernel_44480\\1229253240.py:5: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = dt.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n"
     ]
    }
   ],
   "source": [
    "json_path, md_path = save_report_files(report)\n",
    "print(f\"Reporte guardado en:\\n- {json_path}\\n- {md_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
